{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender mitigated word embedding using adversarial feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project, we will try to mitiage the gender information in word embedding, based on [GloVe](https://nlp.stanford.edu/projects/glove/) and [Adversarial feature learning](https://arxiv.org/abs/1705.11122)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 GloVe Model\n",
    "refer to [this](https://github.com/kefirski/pytorch_GloVe/blob/master/GloVe/glove.py) and [this](https://github.com/2014mchidamb/TorchGlove/blob/master/glove.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal\n",
    "import torch.optim as optim\n",
    "import os, sys, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, co_oc, embedding_size, x_max = 100, alpha = 0.75):\n",
    "        \"\"\"\n",
    "        param: co_oc: co-occurrence ndarray\n",
    "        \"\"\"\n",
    "        super(GloVe, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        '''co_oc matrix'''\n",
    "        self.co_oc = co_oc + 1.0\n",
    "        self.vocabulary_size,_ = co_oc.shape\n",
    "\n",
    "        self.in_embed = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.in_embed.weight = xavier_normal(self.in_embed.weight) #normalize\n",
    "        \n",
    "        self.in_bias = nn.Embedding(self.vocabulary_size, 1) #bias.shape =(vocabularySize,1)\n",
    "        self.in_bias.weight = xavier_normal(self.in_bias.weight)\n",
    "        \n",
    "        self.out_embed = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.out_embed.weight = xavier_normal(self.out_embed.weight)\n",
    "        \n",
    "        self.out_bias = nn.Embedding(self.vocabulary_size, 1)\n",
    "        self.out_bias.weight = xavier_normal(self.out_bias.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_input, batch_output):\n",
    "        \"\"\"\n",
    "        return the loss\n",
    "        \"\"\"\n",
    "        assert len(batch_input) == len(batch_output)\n",
    "        \n",
    "        batch_size = len(batch_input)\n",
    "    \n",
    "        # ZL: What's this stmt mean?\n",
    "        co_occurences = np.array([self.co_oc[batch_input[i], batch_output[i]] for i in range(batch_size)])\n",
    "        weights = np.array([self._weight(var) for var in co_occurences])\n",
    "        \n",
    "        # Variable can do backpropagation\n",
    "        co_occurences = Variable(t.from_numpy(co_occurences)).float() \n",
    "        weights = Variable(t.from_numpy(weights)).float()\n",
    "        \n",
    "        batch_input = Variable(t.from_numpy(batch_input))\n",
    "        batch_output = Variable(t.from_numpy(batch_output))\n",
    "        \n",
    "        input_embed = self.in_embed(batch_input)\n",
    "        output_embed = self.out_embed(batch_output)\n",
    "        input_bias = self.in_bias(batch_input)\n",
    "        output_bias = self.out_bias(batch_output)\n",
    "        \n",
    "        loss = (t.pow(\n",
    "            ((input_embed * output_embed).sum(1) + input_bias + output_bias).squeeze(1) - t.log(co_occurences), 2\n",
    "        ) * weights).sum() / batch_size\n",
    "        \n",
    "        return loss \n",
    "    \n",
    "    def _weight(self, x):\n",
    "        return 1 if x > self.x_max else (x / self.x_max) ** self.alpha\n",
    "    \n",
    "    def embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy() + self.out_embed.weight.data.cpu().numpy()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a random batch\n",
    "def get_batch(vocab_size, batch_size):\n",
    "    in_index   = np.random.choice(np.arange(vocab_size), size = batch_size, replace = False)\n",
    "    out_index  = np.random.choice(np.arange(vocab_size), size = batch_size, replace = False)\n",
    "    return in_index, out_index\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text size: 4250\n",
      "vocabulary_size: 1533\n"
     ]
    }
   ],
   "source": [
    "context_size = 3\n",
    "words_file = 'test.txt'\n",
    "with open(words_file, 'r') as f:\n",
    "    text = f.read().lower()\n",
    "\n",
    "# word_list = word_tokenize(text)\n",
    "word_list = text.split(\" \")[:1700500]\n",
    "text_size = len(word_list)\n",
    "print(\"text size:\", text_size)\n",
    "vocab = np.unique(word_list)\n",
    "vocabulary_size = len(vocab)\n",
    "print(\"vocabulary_size:\", vocabulary_size)\n",
    "word2ind = {word:ind for ind,word in enumerate(vocab)}\n",
    "embedding_size = 100\n",
    "word2ind['UNK'] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Actuallly not in use\n",
    "import collections\n",
    "# vocabulary_size = 50000\n",
    "\n",
    "def read_data(filename):\n",
    "    with open(filename) as f:\n",
    "        data = f.read().split()\n",
    "    return data\n",
    "\n",
    "def build_dataset(filename, vocabulary_size):\n",
    "        \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "        vocabulary = read_data(filename)\n",
    "        \n",
    "        print('--  Data Size:', len(vocabulary))\n",
    "        print('--  Vocabulary size:', vocabulary_size)\n",
    "        count = [['UNK', -1]]\n",
    "        count.extend(collections.Counter(vocabulary).most_common(vocabulary_size - 1))\n",
    "        word2id = dict()\n",
    "        for (index,word) in enumerate(count):\n",
    "            word2id[word[0]] = index\n",
    "        data = []\n",
    "        unk_count = 0\n",
    "        for word in vocabulary:\n",
    "            index = word2id.get(word,0)\n",
    "            if index == 0:\n",
    "                unk_count += 1\n",
    "            data.append(index)\n",
    "        count[0][1] = unk_count\n",
    "        id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "        return data, count, word2id, id2word\n",
    "    \n",
    "# data, count, word2id, id2word = build_dataset(words_file, vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_co_oc_matrix(vocabulary_size):   \n",
    "    comat = np.zeros((vocabulary_size, vocabulary_size))\n",
    "    for i in range(text_size): #main word\n",
    "        left_context_ids = [word2ind[ind] for ind in word_list[max(0, i - context_size): i]]  #left context\n",
    "        right_context_ids = [word2ind[ind] for ind in word_list[i+1: min(i+context_size+1, text_size)]] #right context\n",
    "        ind = word2ind[word_list[i]]\n",
    "\n",
    "        for left_ind, lind in enumerate(left_context_ids):\n",
    "            comat[ind, lind] += 1./(len(left_context_ids) - left_ind) #symmetrically\n",
    "            \n",
    "        for right_ind, rind in enumerate(right_context_ids):\n",
    "            comat[ind, rind] += 1./(right_ind + 1)\n",
    "            \n",
    "    return comat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_GloVe(co_oc_matrix, embeding_size, batch_size = 50, iterations = 1000):\n",
    "    glove = GloVe(co_oc_matrix, embeding_size)\n",
    "    optimizer = optim.SGD(glove.parameters(), 0.01)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        avg_loss = 0.0\n",
    "        for batch in range(text_size// batch_size):\n",
    "            in_data, out_data = get_batch(len(co_oc_matrix), batch_size) #?? for i in len(batch)\n",
    "        \n",
    "            loss = glove(in_data, out_data)\n",
    "            avg_loss += loss\n",
    "        \n",
    "        print(\"%s-epoch, mean loss = %s\"%(str(i),str(loss.data[0])))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    word_embeddings = glove.embeddings()\n",
    "    \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[59905 28439 50506 33090 46854  9812 29110 26999 31240 23157 10693 36002\n",
      " 20899 57200 27047  9053 49572 23003 61864 47391 15887 38609 38624 13623\n",
      " 32896 43843 20514 37153 65182 21056 63186 14492 21318 68954 32524  4286\n",
      " 45650 19153 36903 22653 66520 19462 29534 25457 60554 39461 10521 62429\n",
      " 15405 15255] [43104 52147 36843 36379 12212 31077 17747 34198 53168 14580 47340 43621\n",
      " 21046 54965 19583 17366 23566 18840 11205 51945 34174 66160  8305 10780\n",
      " 70213 56895 23268 29139 69077 32958  5384 53434 37211  2365 37216 13405\n",
      " 40957 53745  6235 63227  5382 53739 11301 50963 37007 65976 12833 16927\n",
      " 31983 65237]\n"
     ]
    }
   ],
   "source": [
    "#test get_batch()\n",
    "np.random.seed(1)\n",
    "in_index, out_index = get_batch(len(comat), 50)\n",
    "print(in_index, out_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1289"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2ind['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1432\n",
      "(70889, 70889)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'woman'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-455705783574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'was'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2ind\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'woman'"
     ]
    }
   ],
   "source": [
    "# comat = get_co_oc_matrix(vocabulary_size)\n",
    "print(word2ind['was'])\n",
    "print(comat.shape)\n",
    "print(word2ind['woman']) # word2ind doesn't have 'woman'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!loss does not work normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-epoch, mean loss = 5.030234024161473e-05\n",
      "1-epoch, mean loss = 2.834257247741334e-05\n",
      "2-epoch, mean loss = 4.5230855903355405e-05\n",
      "3-epoch, mean loss = 5.2493724069790915e-05\n",
      "4-epoch, mean loss = 3.600874697440304e-05\n",
      "5-epoch, mean loss = 2.431717985018622e-05\n",
      "6-epoch, mean loss = 5.87908252782654e-05\n",
      "7-epoch, mean loss = 3.603397999540903e-05\n",
      "8-epoch, mean loss = 0.0033349920995533466\n",
      "9-epoch, mean loss = 4.6180277422536165e-05\n",
      "10-epoch, mean loss = 1.3806435163132846e-05\n",
      "11-epoch, mean loss = 3.1271250918507576e-05\n",
      "12-epoch, mean loss = 3.887403727276251e-05\n",
      "13-epoch, mean loss = 4.2675128497648984e-05\n",
      "14-epoch, mean loss = 2.3623808374395594e-05\n",
      "15-epoch, mean loss = 2.925370426964946e-05\n",
      "16-epoch, mean loss = 6.123475031927228e-05\n",
      "17-epoch, mean loss = 1.9422164768911898e-05\n",
      "18-epoch, mean loss = 5.567950938711874e-05\n",
      "19-epoch, mean loss = 4.8355392209487036e-05\n",
      "20-epoch, mean loss = 2.4777084036031738e-05\n",
      "21-epoch, mean loss = 4.488590275286697e-05\n",
      "22-epoch, mean loss = 3.544255014276132e-05\n",
      "23-epoch, mean loss = 3.3375767088728026e-05\n",
      "24-epoch, mean loss = 4.147682921029627e-05\n",
      "25-epoch, mean loss = 3.226022090530023e-05\n",
      "26-epoch, mean loss = 5.295310984365642e-05\n",
      "27-epoch, mean loss = 7.467117393389344e-05\n",
      "28-epoch, mean loss = 3.940653914469294e-05\n",
      "29-epoch, mean loss = 3.723122790688649e-05\n",
      "30-epoch, mean loss = 4.6242133976193145e-05\n",
      "31-epoch, mean loss = 3.807948451139964e-05\n",
      "32-epoch, mean loss = 2.168752507714089e-05\n",
      "33-epoch, mean loss = 5.1605369662865996e-05\n",
      "34-epoch, mean loss = 3.490479866741225e-05\n",
      "35-epoch, mean loss = 3.192260192008689e-05\n",
      "36-epoch, mean loss = 4.7582700062775984e-05\n",
      "37-epoch, mean loss = 3.1741401471663266e-05\n",
      "38-epoch, mean loss = 3.232215385651216e-05\n",
      "39-epoch, mean loss = 2.0579174815793522e-05\n",
      "40-epoch, mean loss = 2.790021608234383e-05\n",
      "41-epoch, mean loss = 3.278044459875673e-05\n",
      "42-epoch, mean loss = 3.597246541175991e-05\n",
      "43-epoch, mean loss = 4.086871194886044e-05\n",
      "44-epoch, mean loss = 6.071879033697769e-05\n",
      "45-epoch, mean loss = 5.5564822105225176e-05\n",
      "46-epoch, mean loss = 0.007067936472594738\n",
      "47-epoch, mean loss = 2.8386082703946158e-05\n",
      "48-epoch, mean loss = 3.777493839152157e-05\n",
      "49-epoch, mean loss = 3.276160714449361e-05\n",
      "50-epoch, mean loss = 4.035034362459555e-05\n",
      "51-epoch, mean loss = 5.136299296282232e-05\n",
      "52-epoch, mean loss = 5.455102655105293e-05\n",
      "53-epoch, mean loss = 4.2865864088525996e-05\n",
      "54-epoch, mean loss = 2.6208648705505766e-05\n",
      "55-epoch, mean loss = 3.290163294877857e-05\n",
      "56-epoch, mean loss = 2.7869977202499285e-05\n",
      "57-epoch, mean loss = 4.2657040467020124e-05\n",
      "58-epoch, mean loss = 3.1620915251551196e-05\n",
      "59-epoch, mean loss = 4.8822141252458096e-05\n",
      "60-epoch, mean loss = 2.371395930822473e-05\n",
      "61-epoch, mean loss = 4.173005072516389e-05\n",
      "62-epoch, mean loss = 3.7095996958669275e-05\n",
      "63-epoch, mean loss = 3.7614983739331365e-05\n",
      "64-epoch, mean loss = 3.191305586369708e-05\n",
      "65-epoch, mean loss = 2.2618749426328577e-05\n",
      "66-epoch, mean loss = 3.676733103930019e-05\n",
      "67-epoch, mean loss = 2.6352758141001686e-05\n",
      "68-epoch, mean loss = 3.940743044950068e-05\n",
      "69-epoch, mean loss = 3.72530885215383e-05\n",
      "70-epoch, mean loss = 4.99043489980977e-05\n",
      "71-epoch, mean loss = 2.8448266675695777e-05\n",
      "72-epoch, mean loss = 4.001085471827537e-05\n",
      "73-epoch, mean loss = 5.462155240820721e-05\n",
      "74-epoch, mean loss = 7.197305239969864e-05\n",
      "75-epoch, mean loss = 2.2862532205181196e-05\n",
      "76-epoch, mean loss = 3.732966069946997e-05\n",
      "77-epoch, mean loss = 4.548166543827392e-05\n",
      "78-epoch, mean loss = 3.32341696775984e-05\n",
      "79-epoch, mean loss = 3.777717211050913e-05\n",
      "80-epoch, mean loss = 3.997846579295583e-05\n",
      "81-epoch, mean loss = 3.8447418774012476e-05\n",
      "82-epoch, mean loss = 4.502656156546436e-05\n",
      "83-epoch, mean loss = 1.914608947117813e-05\n",
      "84-epoch, mean loss = 2.7720298021449707e-05\n",
      "85-epoch, mean loss = 3.916768400813453e-05\n",
      "86-epoch, mean loss = 5.3440453484654427e-05\n",
      "87-epoch, mean loss = 4.216997331241146e-05\n",
      "88-epoch, mean loss = 5.4238742450252175e-05\n",
      "89-epoch, mean loss = 2.5912715500453487e-05\n",
      "90-epoch, mean loss = 2.9302254915819503e-05\n",
      "91-epoch, mean loss = 2.4359611416002735e-05\n",
      "92-epoch, mean loss = 3.3147902286145836e-05\n",
      "93-epoch, mean loss = 2.5513669243082404e-05\n",
      "94-epoch, mean loss = 4.719182470580563e-05\n",
      "95-epoch, mean loss = 2.2847700165584683e-05\n",
      "96-epoch, mean loss = 2.5045079382834956e-05\n",
      "97-epoch, mean loss = 2.3297681764233857e-05\n",
      "98-epoch, mean loss = 5.740095366491005e-05\n",
      "99-epoch, mean loss = 5.6516269978601485e-05\n",
      "100-epoch, mean loss = 4.759682997246273e-05\n",
      "101-epoch, mean loss = 3.4148637496400625e-05\n",
      "102-epoch, mean loss = 2.5643235858296975e-05\n",
      "103-epoch, mean loss = 3.7293553759809583e-05\n",
      "104-epoch, mean loss = 4.0070925024338067e-05\n",
      "105-epoch, mean loss = 4.535214975476265e-05\n",
      "106-epoch, mean loss = 2.2943702788325027e-05\n",
      "107-epoch, mean loss = 3.815202944679186e-05\n",
      "108-epoch, mean loss = 4.36554109910503e-05\n",
      "109-epoch, mean loss = 2.694619797694031e-05\n",
      "110-epoch, mean loss = 2.1649047994287685e-05\n",
      "111-epoch, mean loss = 4.004295988124795e-05\n",
      "112-epoch, mean loss = 4.141679164604284e-05\n",
      "113-epoch, mean loss = 1.7997703253058717e-05\n",
      "114-epoch, mean loss = 0.003258661599829793\n",
      "115-epoch, mean loss = 3.599869160098024e-05\n",
      "116-epoch, mean loss = 5.3138548537390307e-05\n",
      "117-epoch, mean loss = 2.9885763069614768e-05\n",
      "118-epoch, mean loss = 3.267103966209106e-05\n",
      "119-epoch, mean loss = 4.396198346512392e-05\n",
      "120-epoch, mean loss = 5.33763159182854e-05\n",
      "121-epoch, mean loss = 3.192477743141353e-05\n",
      "122-epoch, mean loss = 2.3577013052999973e-05\n",
      "123-epoch, mean loss = 2.8824466426158324e-05\n",
      "124-epoch, mean loss = 2.8261099942028522e-05\n",
      "125-epoch, mean loss = 4.187983722658828e-05\n",
      "126-epoch, mean loss = 3.0483410228043795e-05\n",
      "127-epoch, mean loss = 2.818148277583532e-05\n",
      "128-epoch, mean loss = 3.2833340810611844e-05\n",
      "129-epoch, mean loss = 4.8478330427315086e-05\n",
      "130-epoch, mean loss = 2.4128103177645244e-05\n",
      "131-epoch, mean loss = 2.2725829694536515e-05\n",
      "132-epoch, mean loss = 2.7226027668803e-05\n",
      "133-epoch, mean loss = 2.869883246603422e-05\n",
      "134-epoch, mean loss = 2.8928741812705994e-05\n",
      "135-epoch, mean loss = 3.798634861595929e-05\n",
      "136-epoch, mean loss = 2.1815863874508068e-05\n",
      "137-epoch, mean loss = 3.007126178999897e-05\n",
      "138-epoch, mean loss = 4.825382347917184e-05\n",
      "139-epoch, mean loss = 2.7190146283828653e-05\n",
      "140-epoch, mean loss = 1.965756746358238e-05\n",
      "141-epoch, mean loss = 2.826240597642027e-05\n",
      "142-epoch, mean loss = 3.223153544240631e-05\n",
      "143-epoch, mean loss = 2.212066829088144e-05\n",
      "144-epoch, mean loss = 4.2766554543050006e-05\n",
      "145-epoch, mean loss = 2.7221778509556316e-05\n",
      "146-epoch, mean loss = 4.522244489635341e-05\n",
      "147-epoch, mean loss = 2.306730493728537e-05\n",
      "148-epoch, mean loss = 2.1280669898260385e-05\n",
      "149-epoch, mean loss = 3.47363566106651e-05\n",
      "150-epoch, mean loss = 3.5091339668724686e-05\n",
      "151-epoch, mean loss = 2.6437821361469105e-05\n",
      "152-epoch, mean loss = 6.199307244969532e-05\n",
      "153-epoch, mean loss = 0.0032618872355669737\n",
      "154-epoch, mean loss = 3.718149309861474e-05\n",
      "155-epoch, mean loss = 3.543819548212923e-05\n",
      "156-epoch, mean loss = 4.166440703556873e-05\n",
      "157-epoch, mean loss = 4.6638029743917286e-05\n",
      "158-epoch, mean loss = 4.542895476333797e-05\n",
      "159-epoch, mean loss = 3.469788862275891e-05\n",
      "160-epoch, mean loss = 2.9816446840413846e-05\n",
      "161-epoch, mean loss = 4.687699401983991e-05\n",
      "162-epoch, mean loss = 4.07627594540827e-05\n",
      "163-epoch, mean loss = 2.9448387067532167e-05\n",
      "164-epoch, mean loss = 3.336420195410028e-05\n",
      "165-epoch, mean loss = 4.413385977386497e-05\n",
      "166-epoch, mean loss = 3.195387762389146e-05\n",
      "167-epoch, mean loss = 2.1843396098120138e-05\n",
      "168-epoch, mean loss = 1.5039173376862891e-05\n",
      "169-epoch, mean loss = 5.6379922170890495e-05\n",
      "170-epoch, mean loss = 4.7134388296399266e-05\n",
      "171-epoch, mean loss = 2.8066113372915424e-05\n",
      "172-epoch, mean loss = 2.408543150522746e-05\n",
      "173-epoch, mean loss = 2.849525844794698e-05\n",
      "174-epoch, mean loss = 3.159082552883774e-05\n",
      "175-epoch, mean loss = 4.258740955265239e-05\n",
      "176-epoch, mean loss = 2.305168891325593e-05\n",
      "177-epoch, mean loss = 1.8316741261514835e-05\n",
      "178-epoch, mean loss = 3.446831397013739e-05\n",
      "179-epoch, mean loss = 1.8250266293762252e-05\n",
      "180-epoch, mean loss = 4.835472282138653e-05\n",
      "181-epoch, mean loss = 4.026112219435163e-05\n",
      "182-epoch, mean loss = 2.0842033336521126e-05\n",
      "183-epoch, mean loss = 4.445333979674615e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184-epoch, mean loss = 3.2974890928016976e-05\n",
      "185-epoch, mean loss = 4.21784607169684e-05\n",
      "186-epoch, mean loss = 3.248563007218763e-05\n",
      "187-epoch, mean loss = 6.554897117894143e-05\n",
      "188-epoch, mean loss = 2.665251122380141e-05\n",
      "189-epoch, mean loss = 2.736884016485419e-05\n",
      "190-epoch, mean loss = 2.2819229343440384e-05\n",
      "191-epoch, mean loss = 5.164226604392752e-05\n",
      "192-epoch, mean loss = 2.965553358080797e-05\n",
      "193-epoch, mean loss = 3.508747249725275e-05\n",
      "194-epoch, mean loss = 4.009225085610524e-05\n",
      "195-epoch, mean loss = 3.2266954804072157e-05\n",
      "196-epoch, mean loss = 3.4527532989159226e-05\n",
      "197-epoch, mean loss = 3.2657037081662565e-05\n",
      "198-epoch, mean loss = 4.2160980228800327e-05\n",
      "199-epoch, mean loss = 4.473894296097569e-05\n",
      "200-epoch, mean loss = 3.200015635229647e-05\n",
      "201-epoch, mean loss = 2.662545557541307e-05\n",
      "202-epoch, mean loss = 3.005926191690378e-05\n",
      "203-epoch, mean loss = 4.563562833936885e-05\n",
      "204-epoch, mean loss = 3.506509892758913e-05\n",
      "205-epoch, mean loss = 4.013781654066406e-05\n",
      "206-epoch, mean loss = 3.3844069548649713e-05\n",
      "207-epoch, mean loss = 2.122425394190941e-05\n",
      "208-epoch, mean loss = 3.226068292860873e-05\n",
      "209-epoch, mean loss = 3.116601874353364e-05\n",
      "210-epoch, mean loss = 3.842189471470192e-05\n",
      "211-epoch, mean loss = 3.117663072771393e-05\n",
      "212-epoch, mean loss = 4.34111243521329e-05\n",
      "213-epoch, mean loss = 3.3321652153972536e-05\n",
      "214-epoch, mean loss = 3.1078739993972704e-05\n",
      "215-epoch, mean loss = 3.388276672922075e-05\n",
      "216-epoch, mean loss = 3.6648176319431514e-05\n",
      "217-epoch, mean loss = 5.204060653341003e-05\n",
      "218-epoch, mean loss = 3.0357265131897293e-05\n",
      "219-epoch, mean loss = 5.9006764786317945e-05\n",
      "220-epoch, mean loss = 1.5118950614123605e-05\n",
      "221-epoch, mean loss = 3.9300586649915203e-05\n",
      "222-epoch, mean loss = 3.967167140217498e-05\n",
      "223-epoch, mean loss = 3.4772598155541345e-05\n",
      "224-epoch, mean loss = 2.132598456228152e-05\n",
      "225-epoch, mean loss = 3.6356537748361006e-05\n",
      "226-epoch, mean loss = 1.874187364592217e-05\n",
      "227-epoch, mean loss = 3.144221045658924e-05\n",
      "228-epoch, mean loss = 2.2296211682260036e-05\n",
      "229-epoch, mean loss = 2.3857337509980425e-05\n",
      "230-epoch, mean loss = 1.0558367648627609e-05\n",
      "231-epoch, mean loss = 4.344547778600827e-05\n",
      "232-epoch, mean loss = 2.5641969841672108e-05\n",
      "233-epoch, mean loss = 4.514914326136932e-05\n",
      "234-epoch, mean loss = 1.9964722014265135e-05\n",
      "235-epoch, mean loss = 4.6406414185184985e-05\n",
      "236-epoch, mean loss = 5.6278135161846876e-05\n",
      "237-epoch, mean loss = 3.230004949728027e-05\n",
      "238-epoch, mean loss = 2.83000754279783e-05\n",
      "239-epoch, mean loss = 2.994476744788699e-05\n",
      "240-epoch, mean loss = 3.269493026891723e-05\n",
      "241-epoch, mean loss = 4.129237640881911e-05\n",
      "242-epoch, mean loss = 1.7877067875815555e-05\n",
      "243-epoch, mean loss = 3.243976243538782e-05\n",
      "244-epoch, mean loss = 3.334436769364402e-05\n",
      "245-epoch, mean loss = 2.537665750423912e-05\n",
      "246-epoch, mean loss = 3.5496181226335466e-05\n",
      "247-epoch, mean loss = 3.767716771108098e-05\n",
      "248-epoch, mean loss = 5.7639768783701584e-05\n",
      "249-epoch, mean loss = 2.1551930331042968e-05\n",
      "250-epoch, mean loss = 2.8569984351634048e-05\n",
      "251-epoch, mean loss = 3.1308318284573033e-05\n",
      "252-epoch, mean loss = 4.281161818653345e-05\n",
      "253-epoch, mean loss = 4.0423641621600837e-05\n",
      "254-epoch, mean loss = 4.3788615585071966e-05\n",
      "255-epoch, mean loss = 4.494003587751649e-05\n",
      "256-epoch, mean loss = 2.3234952095663175e-05\n",
      "257-epoch, mean loss = 2.7537544156075455e-05\n",
      "258-epoch, mean loss = 2.241013316961471e-05\n",
      "259-epoch, mean loss = 4.8359172069467604e-05\n",
      "260-epoch, mean loss = 4.0071354305837303e-05\n",
      "261-epoch, mean loss = 5.037485243519768e-05\n",
      "262-epoch, mean loss = 5.037745722802356e-05\n",
      "263-epoch, mean loss = 3.7992154830135405e-05\n",
      "264-epoch, mean loss = 4.1076687921304256e-05\n",
      "265-epoch, mean loss = 3.501946048345417e-05\n",
      "266-epoch, mean loss = 5.842379687237553e-05\n",
      "267-epoch, mean loss = 3.076472057728097e-05\n",
      "268-epoch, mean loss = 3.189919880242087e-05\n",
      "269-epoch, mean loss = 3.603520235628821e-05\n",
      "270-epoch, mean loss = 4.424775397637859e-05\n",
      "271-epoch, mean loss = 2.2098500267020427e-05\n",
      "272-epoch, mean loss = 5.5601860367460176e-05\n",
      "273-epoch, mean loss = 3.9001432014629245e-05\n",
      "274-epoch, mean loss = 4.8414887714898214e-05\n",
      "275-epoch, mean loss = 4.404101491672918e-05\n",
      "276-epoch, mean loss = 3.765946166822687e-05\n",
      "277-epoch, mean loss = 3.529902460286394e-05\n",
      "278-epoch, mean loss = 2.0294781279517338e-05\n",
      "279-epoch, mean loss = 5.87726644880604e-05\n",
      "280-epoch, mean loss = 3.817736796918325e-05\n",
      "281-epoch, mean loss = 0.02553626336157322\n",
      "282-epoch, mean loss = 6.319829117273912e-05\n",
      "283-epoch, mean loss = 4.5368091377895325e-05\n",
      "284-epoch, mean loss = 3.158771141897887e-05\n",
      "285-epoch, mean loss = 3.5459281207295135e-05\n",
      "286-epoch, mean loss = 3.68601868103724e-05\n",
      "287-epoch, mean loss = 4.3717802327591926e-05\n",
      "288-epoch, mean loss = 2.9085795176797546e-05\n",
      "289-epoch, mean loss = 3.053350519621745e-05\n",
      "290-epoch, mean loss = 2.6760468244901858e-05\n",
      "291-epoch, mean loss = 3.545988511177711e-05\n",
      "292-epoch, mean loss = 3.7323025026125833e-05\n",
      "293-epoch, mean loss = 3.145008304272778e-05\n",
      "294-epoch, mean loss = 2.960452184197493e-05\n",
      "295-epoch, mean loss = 2.2160304069984704e-05\n",
      "296-epoch, mean loss = 0.043781861662864685\n",
      "297-epoch, mean loss = 4.576844730763696e-05\n",
      "298-epoch, mean loss = 3.922862742911093e-05\n",
      "299-epoch, mean loss = 3.656812259578146e-05\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = train_GloVe(comat, 50, 20, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Adversarial Feature Learning\n",
    "\n",
    "refer to [this](https://github.com/github-pengge/adversarial_invariance_feature_learning) code and [paper](https://arxiv.org/pdf/1705.11122.pdf)\n",
    "\n",
    "Replace the last term in Eq.(3) to the GloVe loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gender_word_path = '/zf2/jz4fu/Github/CS269/debiaswe/data'\n",
    "gender_word_file = '/zf2/jz4fu/Github/CS269/debiaswe/data/gender_specific_full.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "gender_words = json.load(open(gender_word_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word2ind is a dict\n",
    "# gender_words_index are all index of gender words\n",
    "gender_words_index = [word2ind[item] if item in word2ind else -1 for item in gender_words  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1441"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gender_words_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.input_size, self.hidden_size, bias = True)\n",
    "        self.linear2 = nn.Linear(self.input_size, self.hidden_size, bias = True)\n",
    "        self.lrelu = nn.LeakyReLU(negative_slope = 0.2)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        #initialize weights\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data.normal_(0, 0.02)\n",
    "                m.bias.data.fill_(0)\n",
    "                \n",
    "    def forward(self, x, s):\n",
    "        x = self.lrelu(self.linear1(x))\n",
    "        score = self.sigmoid(t.dot(self.linear2(s), x))\n",
    "        \n",
    "        return score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Adversarial_FL(object):\n",
    "    def __init__(self, D, word_embeddings, cuda = True):\n",
    "        self.D = D\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.cuda = cuda\n",
    "        self.embedding_size = embedding_size\n",
    "        self.gender_index = [x for x in gender_words_index if x >= 0]\n",
    "        \n",
    "        if self.cuda:\n",
    "            self.D.cuda()\n",
    "            \n",
    "    def train(self, lr = 1e-3, batch_size = 50, epochs = 1000):\n",
    "        s = self.get_s(word_embeddings)\n",
    "        pred_criterion = nn.BCELoss()\n",
    "        \n",
    "        if self.cuda:\n",
    "            s = s.cuda()\n",
    "            pred_criterion = pred_criterion.cuda()\n",
    "            \n",
    "        optimizer_D = optim.Adam(self.D.parameters(), lr = lr, betas = (0.5, 0.999))        \n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            begin_time = time.time()\n",
    "            avg_loss = 0.\n",
    "            # Load data (word embedding)\n",
    "            for i in range(text_size//batch_size):\n",
    "                batch_id = t.randn(vocabulary_size, batch_size).int() #type(dtype)\n",
    "                h = Variable(t.FloatTensor(self.word_embeddings[batch_id]).cuda()) #our word embedding\n",
    "                label_s = t.from_numpy([1 if b_id in gender_words_index else 0 for b_id in batch_id])\n",
    "                pred_s = D(h.detach(), s)\n",
    "                D_loss = pred_criterion(pred_s, label_s)  #???How to get pred_s\n",
    "                avg_loss += D_loss / batch_size\n",
    "            print(\"%s-epoch, mean loss = %s\"%(str(epoch),str(avg_loss.data[0])))\n",
    "            D_loss.backward()\n",
    "            optimizer_D.step()\n",
    "        \n",
    "    def get_s(self, word_embeddings):\n",
    "        \"\"\"\n",
    "        Get $s$ from 14 definitional pairs of words. \n",
    "        \n",
    "        Take all differences between the embeddings of the word pairs.\n",
    "        Use PCA to extract the 1st (i.e. largest) components as $s$.\n",
    "        \"\"\"\n",
    "        def_pairs = json.load(open('../debiaswe/data/definitional_pairs.json'))\n",
    "        def_pairs_ind = [[word2ind[pair[0]], word2ind[pair[1]]] \n",
    "                         for pair in def_pairs ]\n",
    "        diff = [word_embeddings[ind_pair[0]] - word_embeddings[ind_pair[1]]\n",
    "                for ind_pair in def_pairs_ind]\n",
    "        pca = decomposition.PCA(n_components = 1)\n",
    "        pca.fit(np.array(diff))\n",
    "        return pca.components_ # This returns the 1st principle component as $s$.\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'woman'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-35c3b5745265>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0madv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdversarial_FL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcuda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0madv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-9968eb931f91>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, lr, batch_size, epochs)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mpred_criterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-9968eb931f91>\u001b[0m in \u001b[0;36mget_s\u001b[0;34m(self, word_embeddings)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdef_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../debiaswe/data/definitional_pairs.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         def_pairs_ind = [[word2ind[pair[0]], word2ind[pair[1]]] \n\u001b[0;32m---> 46\u001b[0;31m                          for pair in def_pairs ]\n\u001b[0m\u001b[1;32m     47\u001b[0m         diff = [word_embeddings[ind_pair[0]] - word_embeddings[ind_pair[1]]\n\u001b[1;32m     48\u001b[0m                 for ind_pair in def_pairs_ind]\n",
      "\u001b[0;32m<ipython-input-86-9968eb931f91>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mdef_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../debiaswe/data/definitional_pairs.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         def_pairs_ind = [[word2ind[pair[0]], word2ind[pair[1]]] \n\u001b[0;32m---> 46\u001b[0;31m                          for pair in def_pairs ]\n\u001b[0m\u001b[1;32m     47\u001b[0m         diff = [word_embeddings[ind_pair[0]] - word_embeddings[ind_pair[1]]\n\u001b[1;32m     48\u001b[0m                 for ind_pair in def_pairs_ind]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'woman'"
     ]
    }
   ],
   "source": [
    "#train the Adversarial_FL\n",
    "hidden_size = 50\n",
    "batch_size = 64\n",
    "training_epochs = 10\n",
    "cuda = True\n",
    "lr = 1e-3\n",
    "D = Discriminator(embedding_size, hidden_size, 1)\n",
    "adv = Adversarial_FL(D, word_embeddings, cuda)\n",
    "adv.train(lr, batch_size, 10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
