{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender mitigated word embedding using adversarial feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project, we will try to mitiage the gender information in word embedding, based on [GloVe](https://nlp.stanford.edu/projects/glove/) and [Adversarial feature learning](https://arxiv.org/abs/1705.11122)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 GloVe Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, co_oc,  embedding_size, x_max = 100, alpha = 0.75):\n",
    "        \"\"\"\n",
    "        co_oc: co-occurrence ndarray\n",
    "        \"\"\"\n",
    "        super(GloVe, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        '''co_oc matrix'''\n",
    "        self.co_oc = co_oc + 1.0\n",
    "        self.vocabulary_size,_ = co_oc.shape\n",
    "\n",
    "        self.in_embed = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.in_embed.weight = xavier_normal(self.in_embed.weight) #normalize\n",
    "        \n",
    "        self.in_bias = nn.Embedding(self.vocabulary_size, 1) #bias.shape =[vocabularySize,1]\n",
    "        self.in_bias.weight = xavier_normal(self.in_bias.weight)\n",
    "        \n",
    "        self.out_embed = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.out_embed.weight = xavier_normal(self.out_embed.weight)\n",
    "        \n",
    "        self.out_bias = nn.Embedding(self.vocabulary_size, 1)\n",
    "        self.out_bias.weight = xavier_normal(self.out_bias.weight)\n",
    "        \n",
    "        \n",
    "    def convert_to_index(self, in_ind, out_ind):\n",
    "        u = min(in_ind, out_ind)\n",
    "        v = max(in_ind, out_ind)\n",
    "        return int((2*self.vocabulary_size -u + 1) * u / 2 + v - u)\n",
    "    \n",
    "    def forward(self, batch_input, batch_output):\n",
    "        \"\"\"\n",
    "        return the loss\n",
    "        \"\"\"\n",
    "        assert len(batch_input) == len(batch_output)\n",
    "        \n",
    "        batch_size = len(batch_input)\n",
    "\n",
    "        co_occurences = np.array([self.co_oc[batch_input[i], batch_output[i]] for i in range(batch_size)])\n",
    "        weights = np.array([self._weight(var) for var in co_occurences])\n",
    "        \n",
    "        co_occurences = Variable(t.from_numpy(co_occurences)).float() #variable can do backpropagation\n",
    "        weights = Variable(t.from_numpy(weights)).float()\n",
    "        \n",
    "        batch_input = Variable(t.from_numpy(batch_input))\n",
    "        batch_output = Variable(t.from_numpy(batch_output))\n",
    "        \n",
    "        input_embed = self.in_embed(batch_input)\n",
    "        output_embed = self.out_embed(batch_output)\n",
    "        input_bias = self.in_bias(batch_input)\n",
    "        output_bias = self.out_bias(batch_output)\n",
    "        \n",
    "        loss = (t.pow(\n",
    "            ((input_embed * output_embed).sum(1) + input_bias + output_bias).squeeze(1) - t.log(co_occurences), 2\n",
    "        ) * weights).sum() / batch_size\n",
    "        \n",
    "        print(loss.data[0])\n",
    "        return loss \n",
    "    \n",
    "    def _weight(self, x):\n",
    "        return 1 if x > self.x_max else (x / self.x_max) ** self.alpha\n",
    "    \n",
    "    def embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy() + self.out_embed.weight.data.cpu().numpy()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(co_oc_matrix, batch_size):\n",
    "    in_index  = np.random.choice(np.arange(len(co_oc_matrix)), size = batch_size, replace = False)\n",
    "    out_index  = np.random.choice(np.arange(len(co_oc_matrix)), size = batch_size, replace = False)\n",
    "    return in_index, out_index\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 306  888  126  486 1022  226  742  872  200  133  309  576  101  705  493\n",
      "  321  553  108  216  919  181  259  528 1048  283 1056  395  385  943  730\n",
      "  673   90  795   49  691  336  298  419   94  156   80  650  886  301  981\n",
      "  148  455   41   65   99] [ 485  201  562  241  193  935  670 1009  510  270   31  933  432 1044  548\n",
      "   93  117  126  108  960  169  203  153  253  396 1089  690  598  429  414\n",
      "  980  313 1075  805 1087  205   86  668  725  716  734  834 1001  695  431\n",
      " 1040  362  111  423   65]\n"
     ]
    }
   ],
   "source": [
    "#test get_batch()\n",
    "np.random.seed(1)\n",
    "in_index, out_index = get_batch(comat, 50)\n",
    "print(in_index, out_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 3\n",
    "def get_co_oc_matrix(words_file):\n",
    "    with open(words_file, 'r') as f:\n",
    "        text = f.read().lower()\n",
    "    word_list = word_tokenize(text)\n",
    "    text_size = len(word_list)\n",
    "    vocab = np.unique(word_list)\n",
    "    vocabulary_size = len(vocab)\n",
    "\n",
    "    word2ind = {word:ind for ind,word in enumerate(vocab)}\n",
    "    comat = np.zeros((vocabulary_size, vocabulary_size))\n",
    "    for i in range(text_size): #main word\n",
    "        for j in range(1, context_size + 1): #all the context words\n",
    "            ind = word2ind[word_list[i]] \n",
    "            if i - j > 0:\n",
    "                lind = word2ind[word_list[i-j]]\n",
    "                comat[ind, lind] += 1.0/j\n",
    "            if i+j < text_size:\n",
    "                rind = word2ind[word_list[i+j]]\n",
    "                comat[ind, rind] += 1./j\n",
    "    co_oc = np.transpose(np.nonzero(comat)) #non-zero index\n",
    "    return comat, co_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_GloVe(co_oc_matrix, embeding_size, batch_size = 50, iterations = 1000):\n",
    "    glove = GloVe(co_oc_matrix, embeding_size)\n",
    "    optimizer = optim.Adagrad(glove.parameters(), 0.05)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        in_data, out_data = get_batch(co_oc_matrix, batch_size)\n",
    "        \n",
    "        loss = glove(in_data, out_data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    word_embeddings = glove.embeddings()\n",
    "    \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115, 1115)\n",
      "(19772, 2)\n"
     ]
    }
   ],
   "source": [
    "comat, co_oc = get_co_oc_matrix('test.txt')\n",
    "print(comat.shape)\n",
    "print(co_oc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = train_GloVe(comat, 5, 10, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
