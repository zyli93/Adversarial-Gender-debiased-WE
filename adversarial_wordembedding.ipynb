{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender mitigated word embedding using adversarial feature learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this project, we will try to mitiage the gender information in word embedding, based on [GloVe](https://nlp.stanford.edu/projects/glove/) and [Adversarial feature learning](https://arxiv.org/abs/1705.11122)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 GloVe Model\n",
    "refer to [this](https://github.com/kefirski/pytorch_GloVe/blob/master/GloVe/glove.py) and [this](https://github.com/2014mchidamb/TorchGlove/blob/master/glove.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.init import xavier_normal\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    def __init__(self, co_oc,  embedding_size, x_max = 100, alpha = 0.75):\n",
    "        \"\"\"\n",
    "        co_oc: co-occurrence ndarray\n",
    "        \"\"\"\n",
    "        super(GloVe, self).__init__()\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "        \n",
    "        '''co_oc matrix'''\n",
    "        self.co_oc = co_oc + 1.0\n",
    "        self.vocabulary_size,_ = co_oc.shape\n",
    "\n",
    "        self.in_embed = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.in_embed.weight = xavier_normal(self.in_embed.weight) #normalize\n",
    "        \n",
    "        self.in_bias = nn.Embedding(self.vocabulary_size, 1) #bias.shape =(vocabularySize,1)\n",
    "        self.in_bias.weight = xavier_normal(self.in_bias.weight)\n",
    "        \n",
    "        self.out_embed = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.out_embed.weight = xavier_normal(self.out_embed.weight)\n",
    "        \n",
    "        self.out_bias = nn.Embedding(self.vocabulary_size, 1)\n",
    "        self.out_bias.weight = xavier_normal(self.out_bias.weight)\n",
    "        \n",
    "    \n",
    "    def forward(self, batch_input, batch_output):\n",
    "        \"\"\"\n",
    "        return the loss\n",
    "        \"\"\"\n",
    "        assert len(batch_input) == len(batch_output)\n",
    "        \n",
    "        batch_size = len(batch_input)\n",
    "\n",
    "        co_occurences = np.array([self.co_oc[batch_input[i], batch_output[i]] for i in range(batch_size)])\n",
    "        weights = np.array([self._weight(var) for var in co_occurences])\n",
    "        \n",
    "        co_occurences = Variable(t.from_numpy(co_occurences)).float() #variable can do backpropagation\n",
    "        weights = Variable(t.from_numpy(weights)).float()\n",
    "        \n",
    "        batch_input = Variable(t.from_numpy(batch_input))\n",
    "        batch_output = Variable(t.from_numpy(batch_output))\n",
    "        \n",
    "        input_embed = self.in_embed(batch_input)\n",
    "        output_embed = self.out_embed(batch_output)\n",
    "        input_bias = self.in_bias(batch_input)\n",
    "        output_bias = self.out_bias(batch_output)\n",
    "        \n",
    "        loss = (t.pow(\n",
    "            ((input_embed * output_embed).sum(1) + input_bias + output_bias).squeeze(1) - t.log(co_occurences), 2\n",
    "        ) * weights).sum() / batch_size\n",
    "        \n",
    "        return loss \n",
    "    \n",
    "    def _weight(self, x):\n",
    "        return 1 if x > self.x_max else (x / self.x_max) ** self.alpha\n",
    "    \n",
    "    def embeddings(self):\n",
    "        return self.in_embed.weight.data.cpu().numpy() + self.out_embed.weight.data.cpu().numpy()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch(vocab_size, batch_size):\n",
    "    in_index  = np.random.choice(np.arange(vocab_size), size = batch_size, replace = False)\n",
    "    out_index  = np.random.choice(np.arange(vocab_size), size = batch_size, replace = False)\n",
    "    return in_index, out_index\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "context_size = 3\n",
    "words_file = 'test.txt'\n",
    "with open(words_file, 'r') as f:\n",
    "    text = f.read().lower()\n",
    "word_list = word_tokenize(text)\n",
    "text_size = len(word_list)\n",
    "vocab = np.unique(word_list)\n",
    "vocabulary_size = len(vocab)\n",
    "word2ind = {word:ind for ind,word in enumerate(vocab)}\n",
    "\n",
    "def get_co_oc_matrix(vocabulary_size):   \n",
    "    comat = np.zeros((vocabulary_size, vocabulary_size))\n",
    "    for i in range(text_size): #main word\n",
    "        left_context_ids = [word2ind[ind] for ind in word_list[max(0, i - context_size): i]]  #left context\n",
    "        right_context_ids = [word2ind[ind] for ind in word_list[i+1: min(i+context_size+1, text_size)]] #right context\n",
    "        ind = word2ind[word_list[i]]\n",
    "\n",
    "        for left_ind, lind in enumerate(left_context_ids):\n",
    "#             print(ind, lind, len(left_context_ids) - left_ind)\n",
    "            comat[ind, lind] += 1./(len(left_context_ids) - left_ind) #symmetrically\n",
    "            comat[lind, ind] += 1./(len(left_context_ids) - left_ind)\n",
    "            \n",
    "        for right_ind, rind in enumerate(right_context_ids):\n",
    "#             print(\"right:\", ind, rind, right_ind + 1)\n",
    "            comat[ind, rind] += 1./(right_ind + 1)\n",
    "            comat[rind, ind] += 1./(right_ind + 1)\n",
    "            \n",
    "    co_oc = np.transpose(np.nonzero(comat)) #non-zero index\n",
    "    return comat, co_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_GloVe(co_oc_matrix, embeding_size, batch_size = 50, iterations = 1000):\n",
    "    glove = GloVe(co_oc_matrix, embeding_size)\n",
    "    optimizer = optim.Adagrad(glove.parameters(), 0.05)\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        in_data, out_data = get_batch(len(co_oc_matrix), batch_size)\n",
    "        \n",
    "        loss = glove(in_data, out_data)\n",
    "        \n",
    "        print(\"%s-epoch, mean loss = %s\"%(str(i),str(loss.data[0])))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    word_embeddings = glove.embeddings()\n",
    "    \n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 306  888  126  486 1022  226  742  872  200  133  309  576  101  705  493\n",
      "  321  553  108  216  919  181  259  528 1048  283 1056  395  385  943  730\n",
      "  673   90  795   49  691  336  298  419   94  156   80  650  886  301  981\n",
      "  148  455   41   65   99] [ 485  201  562  241  193  935  670 1009  510  270   31  933  432 1044  548\n",
      "   93  117  126  108  960  169  203  153  253  396 1089  690  598  429  414\n",
      "  980  313 1075  805 1087  205   86  668  725  716  734  834 1001  695  431\n",
      " 1040  362  111  423   65]\n"
     ]
    }
   ],
   "source": [
    "#test get_batch()\n",
    "np.random.seed(1)\n",
    "in_index, out_index = get_batch(len(comat), 50)\n",
    "print(in_index, out_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2ind['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1053\n",
      "(1115, 1115)\n"
     ]
    }
   ],
   "source": [
    "comat, co_oc = get_co_oc_matrix(vocabulary_size)\n",
    "print(word2ind['was'])\n",
    "print(comat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### !!!loss does not work normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-epoch, mean loss = 0.0014853744069114327\n",
      "1-epoch, mean loss = 0.0027708634734153748\n",
      "2-epoch, mean loss = 0.0031356457620859146\n",
      "3-epoch, mean loss = 0.0034540309570729733\n",
      "4-epoch, mean loss = 0.002433686051517725\n",
      "5-epoch, mean loss = 0.0019925106316804886\n",
      "6-epoch, mean loss = 0.28705134987831116\n",
      "7-epoch, mean loss = 0.0014464030973613262\n",
      "8-epoch, mean loss = 0.0010627468582242727\n",
      "9-epoch, mean loss = 0.09177442640066147\n",
      "10-epoch, mean loss = 0.0019083479419350624\n",
      "11-epoch, mean loss = 0.0882575660943985\n",
      "12-epoch, mean loss = 0.0021228003315627575\n",
      "13-epoch, mean loss = 0.08882273733615875\n",
      "14-epoch, mean loss = 0.0888444110751152\n",
      "15-epoch, mean loss = 0.001352024031803012\n",
      "16-epoch, mean loss = 0.0018375886138528585\n",
      "17-epoch, mean loss = 0.18161331117153168\n",
      "18-epoch, mean loss = 0.0023743044584989548\n",
      "19-epoch, mean loss = 0.0040374817326664925\n",
      "20-epoch, mean loss = 0.08394147455692291\n",
      "21-epoch, mean loss = 0.002311627846211195\n",
      "22-epoch, mean loss = 0.014286595396697521\n",
      "23-epoch, mean loss = 0.08492390811443329\n",
      "24-epoch, mean loss = 0.029907044023275375\n",
      "25-epoch, mean loss = 0.02602839469909668\n",
      "26-epoch, mean loss = 0.0032000523060560226\n",
      "27-epoch, mean loss = 0.002285529160872102\n",
      "28-epoch, mean loss = 0.045475102961063385\n",
      "29-epoch, mean loss = 0.0028263586573302746\n",
      "30-epoch, mean loss = 0.002124191028997302\n",
      "31-epoch, mean loss = 0.004077367018908262\n",
      "32-epoch, mean loss = 0.0030031907372176647\n",
      "33-epoch, mean loss = 0.02731034718453884\n",
      "34-epoch, mean loss = 0.5353254675865173\n",
      "35-epoch, mean loss = 0.0024086995981633663\n",
      "36-epoch, mean loss = 0.003786827204748988\n",
      "37-epoch, mean loss = 0.002738810144364834\n",
      "38-epoch, mean loss = 0.002589710522443056\n",
      "39-epoch, mean loss = 0.0038333707489073277\n",
      "40-epoch, mean loss = 0.0021454680245369673\n",
      "41-epoch, mean loss = 0.0026439374778419733\n",
      "42-epoch, mean loss = 0.002350317547097802\n",
      "43-epoch, mean loss = 0.004825933370739222\n",
      "44-epoch, mean loss = 0.013202394358813763\n",
      "45-epoch, mean loss = 0.0032945622224360704\n",
      "46-epoch, mean loss = 0.0031150244176387787\n",
      "47-epoch, mean loss = 0.0036826315335929394\n",
      "48-epoch, mean loss = 0.002021424239501357\n",
      "49-epoch, mean loss = 0.0016727361362427473\n",
      "50-epoch, mean loss = 0.003230500966310501\n",
      "51-epoch, mean loss = 0.003736210288479924\n",
      "52-epoch, mean loss = 0.08880316466093063\n",
      "53-epoch, mean loss = 0.0016057336470112205\n",
      "54-epoch, mean loss = 0.0025046716909855604\n",
      "55-epoch, mean loss = 0.15299361944198608\n",
      "56-epoch, mean loss = 0.0034776958636939526\n",
      "57-epoch, mean loss = 0.0029866392724215984\n",
      "58-epoch, mean loss = 0.025988439098000526\n",
      "59-epoch, mean loss = 0.0035074923653155565\n",
      "60-epoch, mean loss = 0.08954804390668869\n",
      "61-epoch, mean loss = 0.0031907535158097744\n",
      "62-epoch, mean loss = 0.00379567826166749\n",
      "63-epoch, mean loss = 0.00347980041988194\n",
      "64-epoch, mean loss = 0.027902554720640182\n",
      "65-epoch, mean loss = 0.18041358888149261\n",
      "66-epoch, mean loss = 0.002402724465355277\n",
      "67-epoch, mean loss = 0.1110759973526001\n",
      "68-epoch, mean loss = 0.10274360328912735\n",
      "69-epoch, mean loss = 0.09091388434171677\n",
      "70-epoch, mean loss = 0.004928906448185444\n",
      "71-epoch, mean loss = 0.0021298001520335674\n",
      "72-epoch, mean loss = 0.08783954381942749\n",
      "73-epoch, mean loss = 0.0018142384942620993\n",
      "74-epoch, mean loss = 0.003971830941736698\n",
      "75-epoch, mean loss = 0.00276686018332839\n",
      "76-epoch, mean loss = 0.002175707370042801\n",
      "77-epoch, mean loss = 0.023474736139178276\n",
      "78-epoch, mean loss = 0.0025151504669338465\n",
      "79-epoch, mean loss = 0.004901059903204441\n",
      "80-epoch, mean loss = 0.013704335317015648\n",
      "81-epoch, mean loss = 0.0024151885882019997\n",
      "82-epoch, mean loss = 0.04044915363192558\n",
      "83-epoch, mean loss = 0.014219780452549458\n",
      "84-epoch, mean loss = 0.00221228483133018\n",
      "85-epoch, mean loss = 0.0021794289350509644\n",
      "86-epoch, mean loss = 0.005258851684629917\n",
      "87-epoch, mean loss = 0.002924881409853697\n",
      "88-epoch, mean loss = 0.004771810490638018\n",
      "89-epoch, mean loss = 0.0018475728575140238\n",
      "90-epoch, mean loss = 0.002953276503831148\n",
      "91-epoch, mean loss = 0.003235200420022011\n",
      "92-epoch, mean loss = 0.0026784734800457954\n",
      "93-epoch, mean loss = 0.0019892086274921894\n",
      "94-epoch, mean loss = 0.0025136584881693125\n",
      "95-epoch, mean loss = 0.002306660171598196\n",
      "96-epoch, mean loss = 0.0030341721139848232\n",
      "97-epoch, mean loss = 0.0020340601913630962\n",
      "98-epoch, mean loss = 0.0025352996308356524\n",
      "99-epoch, mean loss = 0.003349360078573227\n",
      "100-epoch, mean loss = 1.0160067081451416\n",
      "101-epoch, mean loss = 0.002517069224268198\n",
      "102-epoch, mean loss = 0.003562840400263667\n",
      "103-epoch, mean loss = 0.0024887369945645332\n",
      "104-epoch, mean loss = 0.027028217911720276\n",
      "105-epoch, mean loss = 0.0026572244241833687\n",
      "106-epoch, mean loss = 0.003278140677139163\n",
      "107-epoch, mean loss = 0.003165819915011525\n",
      "108-epoch, mean loss = 0.0018099152948707342\n",
      "109-epoch, mean loss = 0.0029902516398578882\n",
      "110-epoch, mean loss = 0.0035049517173320055\n",
      "111-epoch, mean loss = 0.0020740660838782787\n",
      "112-epoch, mean loss = 0.02631165087223053\n",
      "113-epoch, mean loss = 0.002836825791746378\n",
      "114-epoch, mean loss = 0.0026392226573079824\n",
      "115-epoch, mean loss = 0.0029958731029182673\n",
      "116-epoch, mean loss = 0.0022308274637907743\n",
      "117-epoch, mean loss = 0.0024709179997444153\n",
      "118-epoch, mean loss = 0.0035447601694613695\n",
      "119-epoch, mean loss = 0.004110689274966717\n",
      "120-epoch, mean loss = 0.003540029749274254\n",
      "121-epoch, mean loss = 0.005751240998506546\n",
      "122-epoch, mean loss = 0.002524515613913536\n",
      "123-epoch, mean loss = 0.027202848345041275\n",
      "124-epoch, mean loss = 0.8241647481918335\n",
      "125-epoch, mean loss = 0.0038864717353135347\n",
      "126-epoch, mean loss = 0.0019867713563144207\n",
      "127-epoch, mean loss = 0.002971538109704852\n",
      "128-epoch, mean loss = 0.003022950142621994\n",
      "129-epoch, mean loss = 0.002540797693654895\n",
      "130-epoch, mean loss = 0.029716551303863525\n",
      "131-epoch, mean loss = 0.0027905856259167194\n",
      "132-epoch, mean loss = 0.024228600785136223\n",
      "133-epoch, mean loss = 0.004151758272200823\n",
      "134-epoch, mean loss = 0.0051473951898515224\n",
      "135-epoch, mean loss = 0.0028445012867450714\n",
      "136-epoch, mean loss = 0.0036546471528708935\n",
      "137-epoch, mean loss = 0.003001471748575568\n",
      "138-epoch, mean loss = 0.018377196043729782\n",
      "139-epoch, mean loss = 0.004349461756646633\n",
      "140-epoch, mean loss = 0.0017116520320996642\n",
      "141-epoch, mean loss = 0.003686764743179083\n",
      "142-epoch, mean loss = 0.0032278888393193483\n",
      "143-epoch, mean loss = 0.0037121656350791454\n",
      "144-epoch, mean loss = 0.0027411137707531452\n",
      "145-epoch, mean loss = 0.1498391330242157\n",
      "146-epoch, mean loss = 0.002742233220487833\n",
      "147-epoch, mean loss = 0.0015200104098767042\n",
      "148-epoch, mean loss = 0.02568196691572666\n",
      "149-epoch, mean loss = 0.002685819286853075\n",
      "150-epoch, mean loss = 0.003903643460944295\n",
      "151-epoch, mean loss = 0.004158950410783291\n",
      "152-epoch, mean loss = 0.0052596889436244965\n",
      "153-epoch, mean loss = 0.004035984165966511\n",
      "154-epoch, mean loss = 0.002390110632404685\n",
      "155-epoch, mean loss = 0.004094778560101986\n",
      "156-epoch, mean loss = 3.5701420307159424\n",
      "157-epoch, mean loss = 0.002975796116515994\n",
      "158-epoch, mean loss = 0.0032382451463490725\n",
      "159-epoch, mean loss = 0.00326854782178998\n",
      "160-epoch, mean loss = 0.0026470981538295746\n",
      "161-epoch, mean loss = 0.002589558716863394\n",
      "162-epoch, mean loss = 0.014615614898502827\n",
      "163-epoch, mean loss = 0.10932149738073349\n",
      "164-epoch, mean loss = 0.0048474580980837345\n",
      "165-epoch, mean loss = 0.00295169441960752\n",
      "166-epoch, mean loss = 0.002234953921288252\n",
      "167-epoch, mean loss = 0.0032113126944750547\n",
      "168-epoch, mean loss = 0.003528458997607231\n",
      "169-epoch, mean loss = 0.0022354000248014927\n",
      "170-epoch, mean loss = 0.0025441343896090984\n",
      "171-epoch, mean loss = 0.0034911860711872578\n",
      "172-epoch, mean loss = 0.004289368167519569\n",
      "173-epoch, mean loss = 0.002285135444253683\n",
      "174-epoch, mean loss = 0.026467707008123398\n",
      "175-epoch, mean loss = 0.0033559154253453016\n",
      "176-epoch, mean loss = 0.002746095648035407\n",
      "177-epoch, mean loss = 0.004085724242031574\n",
      "178-epoch, mean loss = 0.0030650096014142036\n",
      "179-epoch, mean loss = 0.0019813929684460163\n",
      "180-epoch, mean loss = 0.002481828909367323\n",
      "181-epoch, mean loss = 0.0036244001239538193\n",
      "182-epoch, mean loss = 0.003256862983107567\n",
      "183-epoch, mean loss = 0.0028929985128343105\n",
      "184-epoch, mean loss = 0.00324394553899765\n",
      "185-epoch, mean loss = 0.0023260940797626972\n",
      "186-epoch, mean loss = 0.002266109921038151\n",
      "187-epoch, mean loss = 0.0030707980040460825\n",
      "188-epoch, mean loss = 0.22917798161506653\n",
      "189-epoch, mean loss = 0.0025018840096890926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190-epoch, mean loss = 0.004451552405953407\n",
      "191-epoch, mean loss = 0.002075175754725933\n",
      "192-epoch, mean loss = 0.0032145585864782333\n",
      "193-epoch, mean loss = 0.003255173098295927\n",
      "194-epoch, mean loss = 0.08811486512422562\n",
      "195-epoch, mean loss = 0.005282385740429163\n",
      "196-epoch, mean loss = 0.00260902545414865\n",
      "197-epoch, mean loss = 0.0028478153981268406\n",
      "198-epoch, mean loss = 0.24867987632751465\n",
      "199-epoch, mean loss = 0.003388919634744525\n",
      "200-epoch, mean loss = 0.0025844776537269354\n",
      "201-epoch, mean loss = 0.0014702752232551575\n",
      "202-epoch, mean loss = 0.0027991468086838722\n",
      "203-epoch, mean loss = 0.037564389407634735\n",
      "204-epoch, mean loss = 0.003589692059904337\n",
      "205-epoch, mean loss = 0.003770689247176051\n",
      "206-epoch, mean loss = 0.12643033266067505\n",
      "207-epoch, mean loss = 0.030492544174194336\n",
      "208-epoch, mean loss = 0.0037364661693573\n",
      "209-epoch, mean loss = 0.0026701653841882944\n",
      "210-epoch, mean loss = 0.0028944690711796284\n",
      "211-epoch, mean loss = 0.003071450162678957\n",
      "212-epoch, mean loss = 0.003829836379736662\n",
      "213-epoch, mean loss = 0.004241395741701126\n",
      "214-epoch, mean loss = 0.003227259498089552\n",
      "215-epoch, mean loss = 0.002916260389611125\n",
      "216-epoch, mean loss = 0.0025276201777160168\n",
      "217-epoch, mean loss = 0.02480078861117363\n",
      "218-epoch, mean loss = 0.0024140351451933384\n",
      "219-epoch, mean loss = 0.002212713472545147\n",
      "220-epoch, mean loss = 0.004404574166983366\n",
      "221-epoch, mean loss = 0.0031683961860835552\n",
      "222-epoch, mean loss = 0.002728440333157778\n",
      "223-epoch, mean loss = 0.0033245659433305264\n",
      "224-epoch, mean loss = 0.0021490766666829586\n",
      "225-epoch, mean loss = 0.003534278366714716\n",
      "226-epoch, mean loss = 0.2747947573661804\n",
      "227-epoch, mean loss = 0.0028163481038063765\n",
      "228-epoch, mean loss = 0.001559851923957467\n",
      "229-epoch, mean loss = 0.001655086292885244\n",
      "230-epoch, mean loss = 0.2053554356098175\n",
      "231-epoch, mean loss = 0.02705010399222374\n",
      "232-epoch, mean loss = 0.0027886582538485527\n",
      "233-epoch, mean loss = 0.0020843842066824436\n",
      "234-epoch, mean loss = 0.11013110727071762\n",
      "235-epoch, mean loss = 0.08080870658159256\n",
      "236-epoch, mean loss = 0.0032431199215352535\n",
      "237-epoch, mean loss = 0.002465421799570322\n",
      "238-epoch, mean loss = 0.0024622927885502577\n",
      "239-epoch, mean loss = 0.002278834581375122\n",
      "240-epoch, mean loss = 0.015182504430413246\n",
      "241-epoch, mean loss = 0.0027397610247135162\n",
      "242-epoch, mean loss = 0.0021819700486958027\n",
      "243-epoch, mean loss = 0.0021539016161113977\n",
      "244-epoch, mean loss = 0.0023131452035158873\n",
      "245-epoch, mean loss = 0.0045220633037388325\n",
      "246-epoch, mean loss = 0.002676494652405381\n",
      "247-epoch, mean loss = 0.002107605803757906\n",
      "248-epoch, mean loss = 0.08512789756059647\n",
      "249-epoch, mean loss = 0.02584485150873661\n",
      "250-epoch, mean loss = 0.0787973627448082\n",
      "251-epoch, mean loss = 0.021120551973581314\n",
      "252-epoch, mean loss = 0.0034366417676210403\n",
      "253-epoch, mean loss = 0.003189199371263385\n",
      "254-epoch, mean loss = 0.002249496290460229\n",
      "255-epoch, mean loss = 0.0029456033371388912\n",
      "256-epoch, mean loss = 0.0021829293109476566\n",
      "257-epoch, mean loss = 0.0030449884943664074\n",
      "258-epoch, mean loss = 0.08408163487911224\n",
      "259-epoch, mean loss = 0.002942262915894389\n",
      "260-epoch, mean loss = 0.002444478450343013\n",
      "261-epoch, mean loss = 0.0029211535584181547\n",
      "262-epoch, mean loss = 0.0026970140170305967\n",
      "263-epoch, mean loss = 0.00229289336130023\n",
      "264-epoch, mean loss = 0.0028329237829893827\n",
      "265-epoch, mean loss = 0.09418372809886932\n",
      "266-epoch, mean loss = 0.0028321524150669575\n",
      "267-epoch, mean loss = 0.015122783370316029\n",
      "268-epoch, mean loss = 0.0019343129824846983\n",
      "269-epoch, mean loss = 0.013491712510585785\n",
      "270-epoch, mean loss = 0.001966092037037015\n",
      "271-epoch, mean loss = 0.002990193199366331\n",
      "272-epoch, mean loss = 0.0034652866888791323\n",
      "273-epoch, mean loss = 0.028798505663871765\n",
      "274-epoch, mean loss = 0.0019548553973436356\n",
      "275-epoch, mean loss = 0.03895387426018715\n",
      "276-epoch, mean loss = 0.00362168881110847\n",
      "277-epoch, mean loss = 0.00429465901106596\n",
      "278-epoch, mean loss = 0.30040302872657776\n",
      "279-epoch, mean loss = 0.0030023488216102123\n",
      "280-epoch, mean loss = 0.16077668964862823\n",
      "281-epoch, mean loss = 0.002383789047598839\n",
      "282-epoch, mean loss = 0.0025120065547525883\n",
      "283-epoch, mean loss = 0.002754305023699999\n",
      "284-epoch, mean loss = 0.08235593140125275\n",
      "285-epoch, mean loss = 0.09201283752918243\n",
      "286-epoch, mean loss = 0.002986692124977708\n",
      "287-epoch, mean loss = 0.00441882386803627\n",
      "288-epoch, mean loss = 0.4222593307495117\n",
      "289-epoch, mean loss = 0.0018506007036194205\n",
      "290-epoch, mean loss = 0.0034104709047824144\n",
      "291-epoch, mean loss = 0.004104237537831068\n",
      "292-epoch, mean loss = 0.0020598038099706173\n",
      "293-epoch, mean loss = 0.002249880228191614\n",
      "294-epoch, mean loss = 0.003311269450932741\n",
      "295-epoch, mean loss = 0.003480811370536685\n",
      "296-epoch, mean loss = 0.0023956072982400656\n",
      "297-epoch, mean loss = 0.0027244933880865574\n",
      "298-epoch, mean loss = 0.015771429985761642\n",
      "299-epoch, mean loss = 0.0029663366731256247\n"
     ]
    }
   ],
   "source": [
    "word_embeddings = train_GloVe(comat, 50, 20, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Adversarial Feature Learning\n",
    "\n",
    "refer to [this](https://github.com/github-pengge/adversarial_invariance_feature_learning) code and [paper](https://arxiv.org/pdf/1705.11122.pdf)\n",
    "\n",
    "Replace the last term in Eq.(3) to the GloVe loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
